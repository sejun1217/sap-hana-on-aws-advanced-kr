[
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "\r이 워크샵을 통해 SAP HANA Quick Starts를 이용하여 SAP HANA High Availability Architecture를 손쉽게 구성하고 HANA DB Cluster 운영을 위한 모니터링 및 유지보수, 이중화 테스트, Cluster 설정 변경 하는 작업에 대한 Best Practice를 배울수 있습니다.\n\r 준비사항  AWS 계정: CloudFormation, EC2, VPC, S3, IAM, Systems Manager 자원을 생성 및 접속 할 수 있는 권한이 필요합니다. AWS 리전: 이번 실습은 N.Virginia (us-east-1) 리전에서 실행합니다. AWS 및 SAP on AWS 에 대한 사전 지식이 필요합니다.(Gereral 및 SAP Immersionday 이수 필수) 브라우저: 최신 버전의 크롬, 파이어폭스를 사용하세요.   Contact Us AWS 서비스에 관한 질문은 AWS Support나 담당 AM을 통해서 문의해 주시고 본 워크샵의 발표자료에 관한 질문 사항은 아래의 email 링크를 통해 문의해 주시면 감사하겠습니다. 이상규 SAP Partner Solutions Architect : leesangg@amazon.com 이진욱 SAP Solutions Architect : jinuklee@amazon.com 김세준 Solutions Architect : sejun@amazon.com 강병수 SAP Partner Dev Manager : byungsk@amazon.com  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/",
	"title": "홈",
	"tags": [],
	"description": "",
	"content": "SAP HANA on AWS Advanced WorkShop   "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab1/",
	"title": "Lab 01. Setup Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Lab 전체에서 사용할 SAP HANA Cluster 를 설치합니다.\n\r Lab Architecture  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab1/lab1-1/",
	"title": "Task 01. SAP HANA Quick Starts",
	"tags": [],
	"description": "",
	"content": "\rSAP HANA Quick Starts를 이용하여 SAP HANA High Availability Architecture를 손쉽게 구성합니다.\n\r 고려사항  이번실습의 각 노드의 OS는 SUSE Linux Enterprise 12 SP4로 구성될 예정입니다. SAP HANA Quick Starts는 Performance Optimized Scenario 로 구성 됩니다. 자세한 사항은 아래 링크를 참고 하시기 바랍니다  SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide       사전작업 : Key Pair 생성 HANA Database 및 Application Servers 에 접속하기 위한 Key Pair 를 생성 합니다.\n AWS Management Console에 로그인 한 뒤 Key Pairs Console 서비스에 접속합니다. Create key pair 버튼을 누릅니다.  Name은 SAP-ImmersionDay-Lab 라고 입력 후 Create key pair 버튼을 누릅니다.  이번 WorkShop은 HANA DB 인스턴스는 Session Manager를 통해 접속하고, Bastion Host(Windows Server 2019)는 key pair를 사용하여 Administrator Password를 복호화 하고 원격 접속 프로그램을 통해 접속할 예정입니다. 다운로드 된 키는 안전한 위치에 보관합니다.(**주의 Lab02에서 사용할 예정입니다.)   SAP HANA Quick Starts SAP HANA Quick Starts는 설치를 위한 다양한 옵션이 있습니다. 본 실습은 새로운 VPC에 HANA Database를 Multi-AZ 기반으로 구성합니다. SAP HANA Quick Starts에 대한 자세한 사항은 옆 링크를 참고하시기 바랍니다.(SAP HANA Quick Start Guide)\n  AWS Management Console에 로그인 한 뒤 SAP HANA Quick Starts 서비스에 접속합니다.\n  리전이 US East(N.Virginia) 인지 확인 합니다. 그리고 Next 버튼을 선택합니다.   Stack name은 이전에 사용했던 name과 동일하지 않는한 변경하지 않습니다.\n  Step2.Specify stack details 은 S4HANA 설치를 위한 VPC, SAP HANA, SAP S/4HANA ABAB Cluster 설정 옵션을 입력하는 단계입니다.\n  Step2.1 Network infrastructure configuration 옵션은 아래와 같이 입력 합니다.\n VPC CIDR(default) : 10.0.0.0/16 Availability Zones for subnet creation(선택) : us-east-1a, us-east-1b CIDR block for Private subnet 1(default) : 10.0.1.0/24 CIDR block for Private subnet 2(default) : 10.0.2.0/24 Enter CIDR block for Public subnet 1(default) : 10.0.3.0/24 Enter CIDR block for Public subnet 2(default) : 10.0.4.0/24     Step2.2 Server and storage configuration 옵션은 아래와 같이 입력 합니다.\n Choose operating system for SAP HANA : SuSELinux12SP4ForSAP-BYOS Enter SUSE BYOS Registration Code : XXXXXXXXXXXXXXXX (자신의 Registration code를 입력 합니다) Choose instance type for SAP HANA : r5.4xlarge Enter Dedicated Host ID : Choose key pair : SAP-ImmersionDay-Lab Choose storage volume type for SAP HANA Log : gp2 Choose storage volume type for SAP HANA Data : gp2 Would you like to turn on encryption? : no     Step2.3 SAP HANA database configuration 옵션은 아래와 같이 입력 합니다.\n Enter domain name : local Enter SAP HANA Primary host name : prihana Enter SAP HANA Secondary host name : sechana Enter SAP HANA system ID : HDB Enter SAP HANA instance number : 00 Enter SAP HANA password : Init12345! Enter SAP HANA Server timezone : UC Enter Amazon S3 URL for SAP HANA software : (e.g s3://sap-immsersionday-hol1/hanadb/) Install SAP HANA software? : Yes     Step2.4 SAP HANA High Availability configuration 옵션은 기본 설정 그대로 사용합니다.\n HANA Primary Site Name : HAP HANA Secondary Site Name : HAS Overlay IP Address : 192.168.1.99 Pacemaker Tag Name : PaceTag     Step2.5 Optional configuration RDP 및 Bastion 설정은 Lab02에서 다룰 예정이므로 변경없이 No 로 선택합니다. 나머지 옵션은 Skip 합니다.   Step2.6 Advanced configuration (Do not modify unless directed by AWS Support) 은 Default 설정을 사용합니다 Next 버튼을 선택합니다.   Step3.Configure stack options 은 Stack 실행 옵션을 설정하는 단계 입니다.\n  Error 발생 시 원활한 Trouble Shooting을 위해 Rollback 옵션을 Disabled 로 변경합니다. 그리고 Next 버튼을 선택합니다.   Step4.Review 은 이전 설정을 확인하는 단계 입니다. 제일 하단 Capabilities 의 두 체크 Box를 선택 후 Create stack 버튼을 선택합니다.   SAP-HANA-HA 스택이 생성 되었습니다. Status가 CREATE_COMPLETE 가 될 때 까지 기다립니다.   Error 가 발생했을 경우 진행하고 있는 Solutions Architect에게 문의 합니다.\n\r Lab01 실습이 종료 되었습니다. 다음은 Lab02를 진행합니다.\n\r "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab2/",
	"title": "Lab 02. Monitor Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Lab01에서 구성한 SAP HANA Cluster를 모니터링 하는 방법과, Transit Gateway를 이용하여 OnPrem 및 다른 VPC에서 Overlay IP에 접속하는 방법을 알아 볼 예정입니다.\n\r Lab Architecture  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/lab4-1/",
	"title": "Task 01. Migrate Cluster",
	"tags": [],
	"description": "",
	"content": "\rTask 01은 crm CLI를 사용하여 마스터 노드에서 실행중인 SAP HANA에서 보조(대기) 노드로 수동 마이그레이션을 수행하는 방법을 보여주는 것입니다. 이 실습을 시작하기 전에 클러스터의 모든 서비스가 오류 없이 정상적으로 실행 중인 상태를 먼저 확인해야합니다.\n\rMigrating a HANA primary 테스트 시나리오   Lab 02. Task 03. HAWK(High Availability Web Konsole)을 수행하여 Web을 통해 Cluster의 상태를 확인합니다.\n  Lab 03. Task 01. Validation of Cluster를 수행하여 CLI를 통해 Cluster의 상태를 확인합니다.\n  Overlay IP 리소스를 마이그레이션 하기 위해 Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Node1(prihana)에서아래 명령을 실행하여 Overray IP 리소스인 res_AWS_IP를 Node2(sechana)로 마이그레이션합니다.\nsudo su - crm resource migrate res_AWS_IP force   \u0026ldquo;migrate\u0026quot;명령을 사용했기 때문에 클러스터는 현재 상태의 Primary Node(prihana)의 RA를 중지하고 Secondary Node를 \u0026ldquo;Master\u0026quot;로 승격합니다. 시스템 복제가 INSYNC (SFAIL) 상태인 경우 Primary룰 마이그레이션하지 말아야합니다. Secondary Node가 새 Primary가 될 때까지 기다립니다.\n\r sechana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 prihana node가 SOK 확인 합니다. (root 유저사용)  sudo su - SAPHanaSR-showAttr   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다. (Lab02 참고)\n Overlay IP Resource가 sechana로 넘어 간것을 확인하실 수 있습니다.     Secondary Node (sechana)에서 클러스터가 Primary Node (prihana)에서시작될 수 있도록 리소스 마이그레이션을 해제합니다.\nsudo su - crm resource unmigrate res_AWS_IP    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab2/lab2-1/",
	"title": "Task 01. Verify Created Resources",
	"tags": [],
	"description": "",
	"content": "\rSAP HANA Quick Starts 로 생성된 중요 리소스들을 확인합니다.\n\r  생성된 SAP HANA Instance를 확인 합니다. AWS Management Console에 로그인 한 뒤 EC2 Console에 접속합니다.\n  HANA DB Primary 와 Secondary 인스턴스가 생성된 것을 확인하실 수 있습니다.   HANA DB Primary 또는 Secondary 인스턴스를 선택하고 아래 Description 탭을 보면 Secondary private ip가 생성된 것을 확인하실 수 있습니다. 이 Secondary IP는 두 노드간 통신하는 데 사용합니다.(다른 노드가 이슈가 있는지 확인하는 용도로 사용)   HANA DB Primary 또는 Secondary 인스턴스를 선택하고 아래 Tags 탭을 보면 PaceTag(Quick Starts에서 옵션으로 입력)가 생성된 것을 확인하 실 수 있습니다. EC2 Instance의 Host name은 자동으로 생성성되기 때문에 Cluster agent는 이 태그를 보고 HANA DB Instance를 구분 합니다.   HANA DB Primary 또는 Secondary 인스턴스를 선택하고 상단에 있는 Connect 버튼을 선택하면 Session Manager 통해 인스턴스에 접속 할 수 있습니다. Connect 버튼을 누르면 인스턴스에 접속합니다. Lab03에서는 Session Manager를 통해 인스턴스를 접속하여 HA 테스트를 진행할 예정입니다.   생성된 VPC 및 Subnets 을 확인 합니다. AWS Management Console에 로그인 한 뒤 Subnets Console에 접속합니다.\n  Private 및 Public Subnet이 각각 두개 씩 생성된 것을 확인하실 수 있습니다.\n  좌 측에 Route Tables 메뉴를 확인하시면. Public 및 Private Subnet에 대한 라우팅을 확인하실 수 있습니다.\n  Private subnet route table 을 선택하고 아래 Routes 탭을 확인하시면 Overlay IP(아래 Connect to Overlay IP 설명 참고)에 대한 라우팅 엔트리를 확인 하실 수 있습니다. Target에 정의 되어 있는 eni를 클릭 하면 현재 Primary DB와 연결되어 있는것을 확인 하실 수 있습니다.(* 참고: 사용자 마다 private ip는 다릅니다.)\n Lab03에서 Secondary DB로 take over 된 이후에 Route Table이 어떻게 변경되는지 확인해 보시기 바랍니다.\n     "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab2/lab2-2/",
	"title": "Task 02. Connect to Overlay IP",
	"tags": [],
	"description": "",
	"content": "\rMulti AZ 환경에서 High Availability Cluster를 구성하게 될 경우, Application Server 및 HANA Database 가 사용하는 VIP는 VPC CIDR 밖에 있는 Overlay IP를 사용합니다. 이것을 다른 VPC 혹은 OnPremise 환경에서 접속하기 위해서는 Transit Gateway를 사용할 수 있습니다. 이번 실습에서는 CloudFormation을 통해 가상의 Custom VPC 환경을 만들고, Transit Gateway를 설정한 다음, Bastion Host통해 HANA Database의 Overlay IP에 연결할 예정입니다.\n\r이번 Task는 총 5 단계로 진행 됩니다.\n Customer VPC 및 Bastion Host 생성 Transit Gateway 생성 및 설정 Instance route table 업데이트 Instance Security Group 업데이트 Bastion Host와 Overlay IP 연결 테스트   Customer VPC 및 Bastion Host 생성 CloudFormation으로 Custom VPC 및 Bastion Host를 생성합니다.\n AWS Management Console에 로그인 한 뒤 CloudFormation for Customer VPC 에 접속합니다. Network Configuration 설정은 기존과 겹치지 않으면 기본 설정을 사용합니다.  화면 아래 Capabilities 의 두개의 체크박스를 선택하고 Create stack 버튼을 누릅니다.  CustomVPC 스택이 생성되었습니다. Status가 CREATE_COMPLTE 될 때까지 기다립니다.  EC2 Instance Console에 접속 합니다. Bastion Host(Windows Sever 2019)가 Custom VPC - Public Subnet 위에 생성된 것을 확인 하실 수 있습니다.    Transit Gateway 생성 및 설정 SAP HANA VPC 및 Custom VPC를 연결하기 위해 Transit Gateway를 생성합니다.\n  Transit Gateways Console에 접속하여 Transit Gateway를 생성하고 HANA DB 인스턴스의 VPC와 Custom VPC를 연결합니다. 그리고 Overlay IP를 연결하기 위한 엔트리를 해당 Transit Gateway의 Routing Table에 등록합니다.\n  Create Transit Gateway 버튼을 누릅니다.   Name Tag에 HANA-TGW 로 입력하고 Create Transit Gateway 버튼을 누릅니다. Close 버튼을 누릅니다.   방금 생성한 HANA-TGW State가 available 이 될때까지 기다립니다.   Transit Gateways Attachment Console에 하여 HANA VPC와, Custom VPC를 연결 합니다. Create Transit Gateway Attachment 버튼을 선택 합니다.   이전에 생성한 HANA-TGW 를 선택하고, Attachment name tag는 HANA VPC 를 입력합니다. VPC ID는 SAP-HANA~ 로 시작하는 것을 선택합니다.   Subnet ID를 Private Subnet 1,2 로 선택하고,우측 하단에 있는 Create attachment 버튼을 누릅니다. Close 버튼을 누릅니다.   다시 Create Transit Gateway Attachment 버튼을 누릅니다.   마찬가지로 HANA-TGW 를 선택하고, Attachment name tag는 Custom VPC 를 입력합니다. VPC ID는 Custom 을 선택합니다.   Subnet ID를 Custom Public Subnet AZ1, AZ2 로 선택하고, 우측 하단에 있는 Create attachment 버튼을 누릅니다. Close 버튼을 누릅니다.   두 연결이 모두 State 가 available 이 될 때까지 기다립니다.(일정 시간 후 Refresh 버튼을 눌러서 State를 확인합니다.)   Transit Gateways Route Tables Console에 접속하여 Overlay IP에 대한 라우팅을 등록 합니다.\n  생성된 라우팅 테이블을 선택하고 아래에 Routes 탭을 선택합니다. Create static route 버튼을 선택 합니다.   CIDR은 192.168.1.99/32 를 입력 합니다.\n  Choose attachment는 HANA VPC 를 선택하고, Create static route 버튼을 누릅니다.   Close 버튼을 누릅니다.\n  Overlay IP에 대한 static route가 등록 된것을 확인하실 수 있습니다.    Instance route table 업데이트 HANA DB Instance 와 Bastion Host가 Transit Gateway를 통해 서로 통신할 수 있도록, 각각의 route table을 업데이트 해줍니다.\n Route Tables Console에 접속합니다. HANA DB Instance 의 route table 부터 업데이트 합니다. Private subnet route table 를 선택하고 아래 Routes 탭을 선택합니다. 다음은 Edit routes 버튼을 선택합니다.  Add routes 버튼을 선택한 다음, Destination을 10.1.0.0/16 로 입력하고, Target을 Transit Gateway 로 선택합니다. HANA-TGW 를 선택하고 Save routes 버튼을 선택합니다.  Close 버튼을 선택합니다. 다음은 Bastion Host의 route table을 업데이트 합니다. Custom Public Routes 를 선택하고 아래 Routes 탭을 선택합니다. 다음은 Edit routes 버튼을 선택합니다.  Add routes 버튼을 선택한 다음, Destination을 10.0.0.0/16 로 입력하고, Target을 Transit Gateway 로 선택합니다. 다음은 HANA-TGW 를 선택합니다. 다시 Add routes 버튼을 선택한 다음, Destination을 192.168.1.99/32 로 입력하고, Target을 Transit Gateway 로 선택합니다. 다음은 HANA-TGW 를 선택합니다. 두 routes 가 등록된 것을 확인하고 Save routes 버튼을 선택합니다.  Close 버튼을 누릅니다.   Instance Security Group 업데이트 Primary 및 Secondary HANA DB 인스턴스의 Security Group에 Bastion Host에 대한 인바운트 트래픽을 허용해 줍니다.\n 우선 Bastion Host의 IP를 확인 합니다. EC2 Instance Console에 접속 합니다. Custom Bastion Host 의 Private IP를 확인 합니다. (e.g 10.1.3.154)  Primary 인스턴스 부터 업데이트 합니다. HANA-HDB-Primary 인스턴스를 선택하고, 아래 Security Groups 링크를 클릭 합니다.  하단의 Inbound rules 탭을 선택하고, Edit inbound rules 버튼을 선택합니다.  제일 아래 Add rule 버튼을 선택하고, Type은 All traffic 를 선택하고 Source는 위에서 찾은 Bastion Host의 CIDR을 입력해 줍니다. (e.g 10.1.3.154/32)  실습 환경이기 때문에, Bastion Host를 All traffic 으로 허용 하였습니다. 실제 환경에서는 필요한 포트만 허용 해주는 것을 권장 드립니다.    제일 아래 Save rules 버튼을 선택합니다. Primary Instance와 동일하게 Secondary Instance에 Security Group에도 업데이트 합니다. 다시 EC2 Instance Console에 접속 합니다. HANA-HDB-Secondary 인스턴스를 선택하고, 아래 Security Groups 링크를 클릭 합니다.  하단의 Inbound rules 탭을 선택하고, Edit inbound rules 버튼을 선택합니다. 제일 아래 Add rule 버튼을 선택하고, Type은 All traffic 를 선택하고 Source는 위에서 찾은 Bastion Host의 CIDR을 입력해 줍니다. (e.g 10.1.3.154/32)  실습 환경이기 때문에, Bastion Host를 All traffic 으로 허용 하였습니다. 실제 환경에서는 필요한 포트만 허용 해주는 것을 권장 드립니다.    제일 아래 Save rules 버튼을 선택합니다.   Bastion Host와 Overlay IP 연결 테스트 Bastion Host에 접속해서 Overlay IP와 연결 가능한지 확인하기 위해 Ping 테스트를 수행 합니다.\n EC2 Instance Console에 접속 합니다.Custom Bastion Host 의 Public IP를 확인 합니다. (e.g 35.173.137.37)  Custom Bastion Host 를 선택하고 Connect 버튼을 누릅니다.  Get Password 를 선택 합니다.  파일 선택 버튼을 선택하고, Lab01에서 저장한 SAP-ImmersionDay-Lab.pem 파일을 선택 합니다.  OS에 따라 아래 화면이 다르게 보일 수 있습니다. 아래 캡처 화면은 MacBook 화면 입니다.    Decrypt Password 버튼을 선택해서, Administrator Password를 확인 합니다.(e.g qRPIUIXXSI5ceUhLuntol%WhZ\u0026amp;WFzCT$)  Close 버튼을 누릅니다. 원격접속 프로그램을 사용하여 Bastion Host에 접속합니다.  Windows OS에서 원격 접속방법 참고 MAC OS에서 원격 접속방법 참고   성공적으로 접속이 되면, Search 창에서 Windows PowerShell을 검색하고, 실행합니다.  PowerShell이 실행되면 Overlay IP와 통신이 되는지 확인하기 위해 ping 192.168.1.99 를 실행 합니다.   PowerShell은 종료하고, 원격접속을 유지한 상태로 Task03을 진행합니다.\n\r "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/lab4-2/",
	"title": "Task 02. Take a node offline",
	"tags": [],
	"description": "",
	"content": "\r이 실습에서는 클러스터 노드에서 실행중인 모든 클러스터 서비스를 보조 노드로 자동 마이그레이션되도록 클러스터 노드를 대기 상태로 만드는 방법을 보여줍니다.\n\rTaking a node offline 테스트 시나리오   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 sechana로 넘어 간것을 확인하실 수 있습니다.     sechana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 prihana node가 SOK 확인 합니다.(root 유저사용)  sudo su - SAPHanaSR-showAttr   아래 명령을 실행하여 Node2 (sechana)를 대기 모드로 설정합니다. (root 유저사용)\nsudo su - crm node standby sechana   클러스터는 Primary SAP HANA 데이터베이스를 중지하고 시스템 복제 동기화가 수행된 경우 Secondary 사이트(Task 01을 정상적으로 수행한 경우 prihana)로 TakeOver를 처리합니다. Secondary 사이트(prihana)가 완전히 새로운 Primary가 될 때까지 기다립니다.\n\r Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 prihana로 넘어 간것을 확인하실 수 있습니다.    Node2 (sechana)에서 리소스가 Node1 (prihana)로 마이그레이션될 때까지 Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 prihana 가 Master 상태인 지 확인합니다. sechana 노드가 standby 상태임을 확인합니다.  sudo su - crm_mon -rfn1   Cluster Node2(sechana)를 온라인으로 설정 (root 유저사용)\nsudo su - crm node online sechana   Cluster Node2(sechana)가 온라인이 될 때까지 Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 online 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAPHana resource 상태를 cleaned up 해줍니다.\ncrm resource cleanup rsc_SAPHana_HDB_HDB00    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab2/lab2-3/",
	"title": "Task 03. HAWK(High Availability Web Konsole)",
	"tags": [],
	"description": "",
	"content": "\rHAWK(High Availability Web Konsole)에 접속하여 HANA DB의 Cluster 상태를 Web 화면으로 확인하실 수 있습니다.\n\r 원격접속 프로그램을 사용하여 Bastion Host(Windows Server 2019)에 접속합니다.  IP와 Administrator의 Password 확인 방법은 Task02를 참고 합니다. Windows OS에서 원격 접속방법 참고 MAC OS에서 원격 접속방법 참고   Windows 버튼을 누르고, Server Manager 에 접속합니다.  Internet Explorer의 보안 설정을 해제하기 위해 Local Server 를 선택하고, IE Enhanced Security Configuration 의 On 을 선택합니다.  Administrator와 Users를 모두 Off 로 변경한 후 OK 버튼을 선택합니다.  Server Manager를 종료합니다. 그리고 Internet Explorer 를 실행합니다.  주소창에 \u0026ldquo;https://192.168.1.99:7630\u0026rdquo; 을 입력합니다. (IP: Overlay IP, Port: 7630) More information 을 선택하고 Go on to the webpage 를 선택합니다.  HAWK Login 페이지가 로딩 됩니다. ID: hacluster , Password: Init12345! 로 입력합니다. (Password는 Lab01의 Quick Starts 옵션에서 SAP HANA password로 입력하였습니다.) Login 버튼을 누릅니다.  Dashboad 메뉴를 선택합니다. 현재 각 노드의 상태와 노드 별 리소스 상태를 확인하실 수 있습니다.   원격접속을 유지한 상태로 Lab03을 진행합니다. HA Test시에 Dashboard 상태를 확인할 예정입니다.\n\r Lab02 실습이 종료 되었습니다. 다음은 Lab03을 진행합니다.\n\r "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/lab4-3/",
	"title": "Task 03. Perform SAP HANA System Replica",
	"tags": [],
	"description": "",
	"content": "\r클러스터 TakeOver 또는 장애 복구가 빠르게 발생하거나 TakeOver가 완료되기 전에 노드가 클러스터에 다시 가입하는 일부 시나리오에서는 SAP HANA 시스템 복제의 일관성이 깨지는 상태가 될 수 있으며 전체 시스템 복제본에 대해 수동으로 강제 동기화를 수행해야 할 수 있습니다.\n\rCluster Maintenance Mode 설정 Primary node인 prihana에서 클러스터를 유지 관리 모드로 설정\n  Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.\n  Session Manager 를 선택하고, Connect 버튼을 누릅니다.\n  Session Manager를 통해 prihana 인스턴스에 접속합니다.\n  클러스터를 유지 관리 모드로 설정 (root 유저사용)\n HANA Database의 시스템 복제 동기화를 수행하기 위해 아래 명령어를 수행하여 클러스터를 유지관리 모드로 설정합니다.  sudo su - crm configure property maintenance-mode=true   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 Cluster 가 unmanaged 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   hdbadm으로 로그인하여 복제 상태를 확인합니다.\nsu – hdbadm HDBSettings.sh systemReplicationStatus.py exit   SAP HANA system replica Secondary node인 sechana에서 SAP HANA system replication 수작업 실행\n EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   hdbadm으로 로그인 하여 Secondary SAP HANA Database를 중지합니다. (hdbadm 유저 사용)\nsu – hdbadm HDB stop exit   전체 복제본으로 새 시스템 복제 등록을 실행합니다. (hdbadm 유저 사용)\nsu – hdbadm hdbnsutil -sr_register --name=HAS --remoteHost=prihana --remoteInstance=00 --replicationMode=sync --force_full_replica exit   Secondary SAP HANA Database 시작합니다. (hdbadm 유저 사용)\nsu – hdbadm HDB start exit   Cluster Maintenance Mode 해제 Primary node인 prihana에서 클러스터를 상태 확인 후 유지 관리 모드를 해제합니다.\n Primary Node에서 전체 복제본 상태를 확인할 수 있습니다. (hdbadm 유저 사용)\nsudo su - hdbadm watch -n 1 HDBSettings.sh systemReplicationStatus.py exit   SAP HANA system replication작업이 완료되었으므로 클러스터의 유지 관리 모드를 해제합니다. (root 유저사용)\nsudo su - crm configure property maintenance-mode=false   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 Cluster 가 정상적인 운영 상태로 변경되었는 지 확인 합니다.  sudo su - crm_mon -rfn1    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/lab4-4/",
	"title": "Task 04. Test Resource Agent",
	"tags": [],
	"description": "",
	"content": "\r클러스터 리소스 에이전트 테스트는 올바른 클러스터 구성을 보장하고 문제를 해결하고 조사하기 위한 클러스터 관리의 중요한 부분입니다. 리소스 에이전트는 노드에 서비스 중단을 일으킬 수 있으므로 실행중인 클러스터 또는 응용 프로그램으로 테스트해서는 안됩니다. 이 테스트를 시작하기 전에 두 노드 모두 클러스터가 중지되었는지 확인하십시오.\n\rHA Cluster 중지 Secondary 노드 Cluster (sechana) 중지 확인 후 Primary Node (prihana) Cluster 중지\nSecondary 노드 Cluster (sechana) 중지\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   sechana Cluster 상태 확인\nsudo su - crm cluster status   sechana Cluster 중지 (root 유저사용)\nsudo su - crm cluster stop   sechana Cluster 상태 확인\nsudo su - crm cluster status   Primary Node (prihana) Cluster 중지 7. EC2 Instance Console에 접속 합니다.\n HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Primary node (prihana) Cluster 상태 확인 (root 유저사용)\nsudo su - crm cluster status   prihana Cluster 중지 (root 유저사용)\nsudo su - crm cluster stop   Primary node (prihana) Cluster 상태 확인 (root 유저사용)\nsudo su - crm cluster status    EC2 Stonith 에이전트 테스트 EC2 Stonith 에이전트를 테스트하면 IAM 정책, AWS CLI 구성 및 EC2 API 엔드 포인트에 대한 연결이 제대로 작동하는지 확인하기 위해 노드 중 하나가 중지됩니다.\n다음과 같은 명령줄에서 상황에 알맞은 매개 변수를 바꿉니다.\nstonith -t external/ec2 profile={AWS-PROFILE} port={CLUSTER-NODE2} tag={AWS-TAG-CONTAINING-HOSTNAME} -T off {CLUSTER-NODE2}\n변수는 아래와 같은 명령으로 확인할 수 있습니다. (root 유저사용)\nsudo su - crm configure show res_AWS_STONITH \r이번 워크샵에서 사용되는 매개변수는 다음과 같습니다.\n : cluster : 기본값을 사용하며 필요한 경우 다른 AWS CLI 프로필 이름으로 바꿉니다.\n : sechana 또는 prihana : 다른 클러스터 노드의 이름 또는 IP 주소입니다.\n : PaceTag : 두 클러스터 노드에 대한 EC2 인스턴스의 태그 이름입니다.\n\r 아래 명령은 클러스터 노드 2를 종료합니다. 명령 실행 중에 오류가 발생하여 작동하지 않는 경우 확인합니다. Primary Node에서 테스트 (root 유저사용)\nsudo su - stonith -t external/ec2 profile=cluster port=sechana tag=PaceTag -T off sechana   EC2 Instance Console에 접속하여 sechana 상태를 확인합니다.\n  HANA-HDB-Secondary 인스턴스가 stopped 상태 입니다. sechana 를 Start 하여 정상 상태인지 확인합니다.   HANA-HDB-Secondary 인스턴스를 선택하고, Action 버튼을 누릅니다. Instance State 에서 Start 를 누릅니다. Yes, Start 버튼을 누릅니다.   HANA-HDB-Secondary 정상적으로 부팅이 되면, prihana 인스턴스에 접속합니다.\n  다음 명령을 사용하여 노드의 상태만 확인할 수 있습니다. Primary Node에서 테스트 (root 유저사용)\nsudo su - stonith -t external/ec2 profile=cluster port=sechana tag=PaceTag -S    오버레이 IP 에이전트 테스트 (aws-vpc-move-ip) 오버레이 IP 에이전트를 테스트하면 테스트가 실행되는 노드의 오버레이 IP 주소가 로컬로 구성되고 그에 따라 VPC 라우팅 테이블이 수정됩니다. 두 시스템 모두에서 한 번에 한 시스템 씩 root User로 다음 명령을 실행합니다.\n변수는 아래와 같은 명령으로 확인할 수 있습니다. (root 유저사용)\nsudo su - crm configure show res_AWS_IP 예시) ip=192.168.1.99 routing_table=rtb-0179c495ec5c99d3f interface=eth0 profile=cluster\nOCF_RESKEY_address=\u0026lt;VIRTUAL-IPV4-ADDRESS\u0026gt;\nOCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt;\nOCF_RESKEY_interface=eth0 OCF_RESKEY_profile=\u0026lt;AWS-PROFILE\u0026gt;\nOCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start\n이번 워크샵에서 사용되는 매개변수는 다음과 같습니다.\n\u0026lt;VIRTUAL-IPV4-ADDRESS\u0026gt; : 192.168.1.99 : ASCS 및 ERS 시스템의 서비스 IP 주소입니다.\n\u0026lt;AWS-ROUTE-TABLE\u0026gt; : 오버레이 IP 주소의 AWS 라우팅 테이블 ID입니다. 이는 모든 실습 사용자마다 다르므로 반드시 확인하여 변경해야합니다. \u0026lt;AWS-PROFILE\u0026gt; : cluster : 필요한 경우 AWS CLI 프로필 이름을 바꿉니다.\n\r 오버레이 IP 에이전트 시작 (root 유저 사용)\n\u0026lt;AWS-ROUTE-TABLE\u0026gt; 확인한 rout table ID로 변경 필요\nsudo su - OCF_RESKEY_address=192.168.1.99 \\ OCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt; \\ OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf \\ OCF_RESKEY_profile=cluster /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip start   IP가 로컬로 설정되었는지 확인하십시오.\n\u0026lt;AWS-ROUTE-TABLE\u0026gt; 확인한 rout table ID로 변경 필요\nsudo su - OCF_RESKEY_address=192.168.1.99 \\ OCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt; \\ OCF_RESKEY_interface=eth0 OCF_RESKEY_profile=cluster \\ OCF_ROOT=/usr/lib/ocf /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip monitor 또는 다음 명령을 통해 Linux에 구성된 IP를 확인할 수도 있습니다.\nip address show   테스트 중이 아닌 노드에서 IP를 확인해봅니다. (예. 현재 prihana에서 오버레이 IP를 테스트 중이라면 sechana에서 확인)\n  오버레이 IP 에이전트 중지 (root 유저 사용)\n\u0026lt;AWS-ROUTE-TABLE\u0026gt; 확인한 rout table ID로 변경 필요\nsudo su - OCF_RESKEY_address=192.168.1.99 \\ OCF_RESKEY_routing_table=\u0026lt;AWS-ROUTE-TABLE\u0026gt; \\ OCF_RESKEY_interface=eth0 OCF_ROOT=/usr/lib/ocf \\ OCF_RESKEY_profile=cluster /usr/lib/ocf/resource.d/suse/aws-vpc-move-ip stop    HA Cluster 시작 테스트가 완료되었으므로 HA Cluster를 시작합니다. Cluster Primary node(prihana) 시작 후 Secondary 노드(sechana) 시작합니다. 클러스터의\nPrimary node(prihana) 시작 20. EC2 Instance Console에 접속 합니다.\n HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Primary node (prihana) Cluster 시작 (root 유저사용)\nsudo su - crm cluster start   Primary node (prihana) Cluster 상태 확인 (root 유저사용)\nsudo su - crm cluster status   Primary node(prihana) 시작 후 Cluster Secondary 노드(sechana) 시작 25. EC2 Instance Console에 접속 합니다. 26. HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다. 27. Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다. Secondary node (sechana) Cluster 시작 (root 유저사용) sudo su - crm cluster start  Secondary node (sechana) Cluster 상태 확인 sudo su - crm cluster status    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/lab4-5/",
	"title": "Task 05 - Upgrade Cluster Software(Optional)",
	"tags": [],
	"description": "",
	"content": "\r이번 실습은 실습 시점의 업데이트 상황에 따라 업데이트가 적용되지 않거나 현재 구성된 HA Cluster의 상태 변경으로 예상하지 못한 테스트 시나리오에 영향이 발생할 가능성이 있습니다. 예상치 못한 영향에 대한 문제 해결이 필요할 경우, 이는 이번 워크샵의 실습 범위를 벗어나게 되며 이후 진행하게 될 Lab 06. Cost Optimized Conifguration 실습과정 진행에도 영향을 주게 됩니다. 모든 실습이 종료된 이후 마지막으로 수행해 보는 것을 권장드립니다.\n\r이 실습에서는 두 개의 클러스터 노드 모두에서 SUSE Linux를 업데이트하는 방법을 보여 줍니다.\n\r노드에 패키지 업데이트를 설치하기 전 반드시 다음 사항들을 확인하십시오. 업데이트가 SUSE Linux Enterprise High Availability Extension or Geo Clustering for SUSE Linux Enterprise High Availability Extension에 속하는 패키지에 영향을 주는가?\n  패키지 업데이트에 재부팅이 필요한 경우에는 소프트웨어 업데이트를 시작하기 전에 보조 노드에서 클러스터 스택을 중지합니다.\nsudo su - crm cluster stop   패키지 업데이트 설치 :\nsudo su - zypper up   사용 가능한 패치 표시 :\nsudo su - zypper lp   사용 가능한 업데이트 표시 :\nsudo su - zypper lu   각 노드에서 클러스터 스택을 시작하거나 서버를 재시작하십시오.\nsudo su - crm cluster start or sudo su - reboot   Lab 02. Task 03. HAWK(High Availability Web Konsole)을 수행하여 Web을 통해 Cluster의 상태를 확인합니다.\n  Lab 03. Task 01. Validation of Cluster를 수행하여 CLI를 통해 Cluster의 상태를 확인합니다.\n   위의 상황이 적용되지 않는 경우 클러스터 스택을 중지 할 필요가 없습니다.\n  이 경우에는 소프트웨어 업데이트를 시작하기 전에 노드를 유지 관리 모드로 전환하십시오. 여기서 \u0026lt;NODE_NAME\u0026gt;은 클러스터 노드의 이름입니다.\ncrm node maintenance \u0026lt;NODE_NAME\u0026gt;\n  패키지 업데이트 설치 :\nsudo su - zypper up   사용 가능한 패치 표시 :\nsudo su - zypper lp   사용 가능한 업데이트 표시 :\nsudo su - zypper lu   작업 완료 후 유지 보수 플래그를 제거하여 노드를 정상 모드로 되돌립니다. 여기서 \u0026lt;NODE_NAME\u0026gt;은 클러스터 노드의 이름입니다.\nsudo su - crm node ready `\u0026lt;NODE_NAME\u0026gt;`   Lab 02. Task 03. HAWK(High Availability Web Konsole)을 수행하여 Web을 통해 Cluster의 상태를 확인합니다.\n  Lab 03. Task 01. Validation of Cluster를 수행하여 CLI를 통해 Cluster의 상태를 확인합니다.\n   Performing a Cluster-wide Rolling Upgrade from one service pack to the next 특정 서비스 팩에서 다음 서비스 팩으로 클러스터 전체의 롤링 업그레이드 수행하는 경우 전체 서비스 팩 업그레이드를 수행하려면 업데이트를 위해 하나의 노드를 오프라인으로 전환하여 업그레이드를 진행한 다음 다시 TakeBack하여 다른 노드에서 업그레이드를 진행하는 절차를 반복해야 합니다.\n\r 업그레이드하려는 노드에 루트로 로그인하고 클러스터 스택을 중지합니다. crm cluster stop SAP 애플리케이션 용 SUSE Linux Enterprise Server의 원하는 대상 버전으로 업그레이드를 수행합니다. 업그레이드가 완료된 노드에서 클러스터 스택을 시작하여 노드가 클러스터에 다시 참여하도록합니다. crm cluster start 다음 노드로 이동하기 전에 \u0026ldquo;crm_mon -rfn1\u0026quot;또는 Hawk2로 클러스터 상태를 확인합니다. 다음 노드를 오프라인으로 전환하고 해당 노드에 대해 절차를 반복합니다.   "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab3/",
	"title": "Lab 03. Test Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 Lab01에서 구성한 SAP HANA Cluster가 테스트 사례에서 정상 동작 하는지 확인하는 방법을 배워 볼 예정입니다.\n\r Lab Architecture 이번 실습에서는 아래 세가지 사례에 대한 테스트를 진행할 예정입니다. 각각의 사례에서 DB가 정상적으로 테이크 오버 되느는지 확인하는 방법을 배울 예정입니다.\n Test Case 01: Stop Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 02: Crash Database(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary) Test Case 03: System crash(Primary \u0026lt;\u0026ndash;\u0026gt; Secondary)    Test Architecture   Secondary(on sechana) to take over as Primary   Secondary(on prihana) to fail back as Primary    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab3/lab3-1/",
	"title": "Task 01. Verify Cluster",
	"tags": [],
	"description": "",
	"content": "\rTask 01은 HA 테스트를 진행하기 전에 클러스터가 정상 상태인지 확인합니다. 인스턴스는 Session Manager를 통해 접속할 예정입니다.\n\r  AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.  SAP HANA System Replication (HSR) 상태 체크를 합니다.   hdbnsutil -sr_state 명령어를 사용하여 상태 체크(hdbadm유저사용)  HAP(prihana) site가 Primary 이고 HAS(sechana) site가 sync 상태인것을 확인 할 수 있습니다.  sudo su - hdbadm hdbnsutil -sr_state  HDBSettings.sh systemReplicationStatus.py 명령어를 사용하여 상태 체크(hdbadm유저사용)  Primary node가 prihana 이고 Secondary node가 sechana 인것을 알 수 있습니다.  HDBSettings.sh systemReplicationStatus.py  SAPHanaSR-showAttr 사용하여 sechana node가 SOK 확인 합니다.(root 유저사용)  exit sudo su - SAPHanaSR-showAttr 5. Cluster 설정을 확인 합니다.\n crm tool은 cluster 설정 및 관리 하는 툴입니다. HA 동작 방식 확인 하려면, PREFER_SITE_TAKEOVER=\u0026quot;true\u0026rdquo; 및 AUTOMATED_REGISTER=\u0026quot;true\u0026rdquo; 두가지 옵션을 확인 홥니다. 둘 다 true 일 경우 Takeover가 자동으로 수행 되고 Secondary로 전환된 노드가 정상일 경우 자동으로 등록된다는 의미 입니다. crm configure show 명령어를 사용하여 상태 체크(root 유저사용)  crm configure show 6. Cluster 상태를 확인 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 prihana 가 Master 상태인지 확인 합니다.  crm_mon -rfn1  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab3/lab3-2/",
	"title": "Task 02. Stop Database",
	"tags": [],
	"description": "",
	"content": "\rTask 02는 클러스터 작업 중에 HANA 데이터베이스가 중지되는 상황에서 클러스터가 어떻게 동작 하는지 배워볼 예정 입니다.\n\r Stop Primary Database on prihana Primary node인 prihana 에서 HDB STOP이 발생 했을 때, Secondary node sechana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   prihana 에 HANA DB를 중지 합니다.(hdbadm유저사용)\nsudo su - hdbadm HDB stop   sechana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 prihana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 sechana로 넘어 간것을 확인하실 수 있습니다.      Stop Primary Database on sechana Primary node인 sechana 에서 HDB STOP이 발생 했을 때, Secondary node prihana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   sechana 에 HANA DB를 중지 합니다.(hdbadm유저사용)\nsudo su - hdbadm HDB stop   prihana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 prihana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 sechana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 prihana로 넘어 간것을 확인하실 수 있습니다.      "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab3/lab3-3/",
	"title": "Task 03. Crash Database",
	"tags": [],
	"description": "",
	"content": "\rTask 03은 HANA 데이터베이스가 운영중인 상황에서 Crash가 발생을 경우 클러스터가 어떻게 동작 하는지 배워볼 예정 입니다.\n\r Crash Primary Database on prihana Primary node인 prihana 에서 DB Crash가 발생 했을 때, Secondary node sechana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   prihana 에 HANA DB를 kill 합니다.(hdbadm유저사용)\nsudo su - hdbadm HDB kill -9   sechana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 prihana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 sechana로 넘어 간것을 확인하실 수 있습니다.      Crash Primary Database on sechana Primary node인 sechana 에서 DB Crash가 발생 했을 때, Secondary node prihana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   sechana 에 HANA DB를 중지 합니다.(hdbadm유저사용)\nsudo su - hdbadm HDB kill -9   prihana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 prihana 가 Master 상태인지 확인 합니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 sechana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n Overlay IP Resource가 prihana로 넘어 간것을 확인하실 수 있습니다.      "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab3/lab3-4/",
	"title": "Task 04. Crash System",
	"tags": [],
	"description": "",
	"content": "\rTask 04는 HANA 데이터베이스가 사용하는 OS가 Crash가 발생했을 경우 클러스터가 어떻게 동작 하는지 배워볼 예정 입니다.\n\r Crash Primary Site Node(prihana) Primary node인 prihana 에서 System Crash가 발생 했을 때, Secondary node sechana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   prihana 에서 fast-reboot을 수행합니다.(root 유저사용)\nsudo su - echo \u0026#39;b\u0026#39; \u0026gt; /proc/sysrq-trigger   sechana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 sechana 가 Master 상태인지 확인 합니다. 이전 Case와 다르게 prihana 가 OFFLINE 상태 입니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 수행 결과 이전과 달리 prihana node가 offline 상태 입니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n 이전 Case와 다르게 prihana 가 offline 상태 입니다.     EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스가 stopped 상태 입니다. prihana 를 Start 하여 정상 상태인지 확인합니다.   HANA-HDB-Primary 인스턴스를 선택하고, Action 버튼을 누릅니다. Instance State 에서 Start 를 누릅니다. Yes, Start 버튼을 누릅니다.   HANA-HDB-Primary 정상적으로 부팅이 되면, sechana 인스턴스에 접속하여, Cluster 및 HSR 상태를 확인 합니다.\n  Cluster의 상태를 체크 합니다.\n 아래 명령어를 수행하여 prihana 가 Slave 상태인지 확인 합니다.(root 유저사용)  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 prihana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n prihana 가 online 상태로 변한것을 확인하실 수 있습니다.      Crash Secondary Site Node (sechana) Primary node인 sechana 에서 System Crash가 발생 했을 때, Secondary node prihana 노드가 Primary node로 정상적으로 전환되는지 확인.   EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 sechana 인스턴스에 접속합니다.   sechana 에서 fast-reboot을 수행합니다.(root 유저사용)\nsudo su - echo \u0026#39;b\u0026#39; \u0026gt; /proc/sysrq-trigger   prihana 에 들어가서 Cluster 및 HSR 상태를 확인합니다.\n  EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Connect 버튼을 누릅니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다. Session Manager를 통해 prihana 인스턴스에 접속합니다.   Cluster의 상태를 체크 합니다.\n crm_mon 은 Cluster의 현재 상태를 제공하는 명령어 입니다.(root 유저사용) 아래 명령어를 수행하여 prihana 가 Master 상태인지 확인 합니다. 이전 Case와 다르게 sechana 가 OFFLINE 상태 입니다.  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 수행 결과 이전과 달리 sechana node가 offline 상태 입니다.(root 유저사용)  SAPHanaSR-showAttr   Take over 후 resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n 이전 Case와 다르게 sechana 가 offline 상태 입니다.     EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스가 stopped 상태 입니다. sechana 를 Start 하여 정상 상태인지 확인합니다.   HANA-HDB-Secondary 인스턴스를 선택하고, Action 버튼을 누릅니다. Instance State 에서 Start 를 누릅니다. Yes, Start 버튼을 누릅니다.   HANA-HDB-Secondary 정상적으로 부팅이 되면, prihana 인스턴스에 접속하여, Cluster 및 HSR 상태를 확인 합니다.\n  Cluster의 상태를 체크 합니다.\n 아래 명령어를 수행하여 sechana 가 Slave 상태인지 확인 합니다.(root 유저사용)  sudo su - crm_mon -rfn1   SAP HANA System Replication (HSR) 상태 체크를 합니다.\n SAPHanaSR-showAttr 사용하여 sechana node가 SOK 확인 합니다.(root 유저사용)  SAPHanaSR-showAttr   resource 상태를 cleaned up 해줍니다.(root 유저사용)\ncrm resource cleanup rsc_SAPHanaTopology_HDB_HDB00   Bastion Host에 접속해서 HAWK 웹의 Dashboard를 확인합니다(Lab02 참고)\n sechana 가 online 상태로 변한것을 확인하실 수 있습니다.      Lab03 실습이 종료 되었습니다. 다음은 Demo.Config Cost Optimized Scenario를 진행합니다.\n\r "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab4/",
	"title": "Lab 04. Admin Cluster",
	"tags": [],
	"description": "",
	"content": "Lab 설명 HA Cluster의 운영 중 필요한 여러 HA Cluster Command들에 대해 알아보도록합니다.\n\r Lab Architecture 이번 실습에서는 Cluster를 운영하면서 필요한 여러 상황에 필요한 HA Cluster 명령어들을 실행해 볼 것입니다.\n 클러스터 마이그레이션 명령을 사용하여 HANA 기본 마이그레이션을 수행 (Primary\u0026ndash;\u0026gt;Secondary) 클러스터 노드를 대기 상태로 변경하여 노드를 오프라인으로 전환 (Secondary\u0026ndash;\u0026gt;Primary) SAP HANA System Replication 수행 Cluster Resource Agents 테스트 SUSE Cluster Software Upgrading 개념과 절차    Test Architecture   Secondary(on sechana) to take over as Primary   Secondary(on prihana) to take back as Primary    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab5/",
	"title": "Lab 05. Change to Cost Optimized Scenario",
	"tags": [],
	"description": "",
	"content": "Demo 설명 QuickStart로 설치된 Performance Optimized Scenario의 SAP HANA DB의 HA 설정을 변경하여 Standby와 QAS 시스템이 같이 있는 Cost Optimized 형태로 구성할 예정입니다. 설정과 관련된 자세한 사항은 아래 링크를 참고하시기 바랍니다\n\r SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide   Demo Architecture  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab5/lab5-1/",
	"title": "Task 01. Change HANA HSR Config",
	"tags": [],
	"description": "",
	"content": "\rTask 01에서는 Standby 시스템에서 QAS 시스템 서비스를 위해 HSR 옵션을 변경하고. Global Memory 설정을 변경할 예정입니다.\n\r   Session Manager를 통해 sechana에 접속합니다.\n AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Secondary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다.     root 유저로 접속 후, Cluster를 Maintenance 모드로 변경하고, HSR 및 Global Memory 설정을 변경합니다\n Cluster를 Maintenance 모드로 변경 합니다.  sudo su - root crm node maintenance prihana crm node maintenance sechana  crm maintenance 상태 확인  crm_mon -rfn1   hdbadm 유저로 접속 후, HDB Stop 하고, Secondary 운영 SAP HANA DB (HDB)의 메모리 자원 사용을 줄이고 preload 옵션 false로 설정합니다.\n (sechana) HDB STOP  su - hdbadm HDB stop  (sechana) global.ini 설정을 변경합니다. VI 편집기로 global.ini 를 오픈 합니다. 그리고 아래 옵션을 추가 합니다.  vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576   Takover 이후에는 해당 설정이 원복 할 수 있도록, SAPHanaSR-Hook 변경 합니다.\n (sechana) root user 접속 후, SAPHanaSR.py 수정하기 전에 스크립트를 백업 합니다.  exit cd /usr/share/SAPHanaSR/ cp -pr SAPHanaSR.py SAPHanaSR.py.default  (sechana) root user 접속 후, SAPHanaSR.py 스크립트를 아래와 같이 2가지를 수정 합니다.  수정 1(AS-Is)  수정 1(To-Be)  수정 2(AS-Is)  수정 2(To-Be)  스크립트는 아래와 같이 수정하시기 바랍니다.  vi SAPHanaSR.py # 수정 1 from hdb_ha_dr.client import HADRBase, Helper import os, time from hdbcli import dbapi dbuser = \u0026#34;SYSTEM\u0026#34; dbpwd = \u0026#34;Init12345!\u0026#34; dbport = 30013 stmnt1 = \u0026#34;ALTER SYSTEM ALTER CONFIGURATION (\u0026#39;global.ini\u0026#39;,\u0026#39;SYSTEM\u0026#39;) UNSET (\u0026#39;memorymanager\u0026#39;,\u0026#39;global_allocation_limit\u0026#39;) WITH RECONFIGURE\u0026#34; stmnt2 = \u0026#34;ALTER SYSTEM ALTER CONFIGURATION (\u0026#39;global.ini\u0026#39;,\u0026#39;SYSTEM\u0026#39;) UNSET (\u0026#39;system_replication\u0026#39;,\u0026#39;preload_column_tables\u0026#39;) WITH RECONFIGURE\u0026#34; # 수정 2 def postTakeover(self, rc, **kwargs): \u0026#34;\u0026#34;\u0026#34;Post takeover hook.\u0026#34;\u0026#34;\u0026#34; self.tracer.info(\u0026#34;%s.postTakeover method called with rc=%s\u0026#34; % (self.__class__.__name__, rc)) if rc == 0: # normal takeover succeeded conn = dbapi.connect(\u0026#39;localhost\u0026#39;, dbport, dbuser, dbpwd) cursor = conn.cursor() cursor.execute(stmnt1) cursor.execute(stmnt2) return 0 elif rc == 1: # waiting for force takeover conn = dbapi.connect(\u0026#39;localhost\u0026#39;, dbport, dbuser, dbpwd) cursor = conn.cursor() cursor.execute(stmnt1) cursor.execute(stmnt2) return 0 elif rc == 2: # error, something went wrong return 0     hdbadm 유저로 접속 후, HDB Start 하고, TAKEOVER TEST 를 수행합니다.\n (sechana)의 Secondary HDB START  su - hdbadm HDB start  (sechana)에서 HSR 상태를 확인합니다.  hdbnsutil -sr_state  (sechana)에서 Takeover를 수행합니다  hdbnsutil -sr_takeover  (sechana)에서 SAPHanaSR.py 에 의해 Takeover 후 global.ini 설정이 원복 되었는지 확인 합니다.  cat /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini  현재 HA Cluster가 maintenance mode이므로 HA Cluster를 사용한 자동 Takeover-Takeback이 불가능한 상태이므로 SAP HANA SR을 원래 상태로 복원하기 위해 아래와 같은 순서로 수작업으로 재설정한다. sechana 연결은 유지하고, Session Manager를 통해 prihana도 접속합니다.  AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Primary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다.    (prihana)에서 hdbadm 유저로 접속 후, REGISTER as seconadry Database 수행  sudo su - hdbadm HDB stop hdbnsutil -sr_register --remoteHost=sechana --remoteInstance=00 --replicationMode=sync --name=HAP --operationMode=logreplay HDB start  (prihana)에서 동기화 상태 확태 확인  hdbnsutil -sr_state  (prihana)에서 takeback  hdbnsutil -sr_takeover  (sechana)에서 hdbadm 유저로 접속 후, REGISTER as seconadry Database 수행  su - hdbadm HDB stop hdbnsutil -sr_register --remoteHost=prihana --remoteInstance=00 --replicationMode=sync --name=HAS --operationMode=logreplay  (sechana)의 global.ini 설정을 변경합니다. VI 편집기로 global.ini 를 오픈 합니다. 그리고 아래 옵션을 추가 합니다.  vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576  (sechana)에서 HDB START (sechana)에서 hdbadm 유저로 접속 후, Secondary HDB START  HDB start    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab5/lab5-2/",
	"title": "Task 02. Install SAP HANA DATABASE (QAS)",
	"tags": [],
	"description": "",
	"content": "\rTask 02에서는 QAS(Non-productive) SAP HANA 데이터베이스를 sechana에 설치합니다. 본 실습에서는 최소한의 기능 검증만을 위한 설치를 진행 할 예정입니다. HANA DB의 설치와 관련된 상세한 내용은 아래 SAP HANA Administration Guide를 참조 하시기 바랍니다.\n\r SAP HANA Administration Guide     Session Manager를 통해 sechana에 접속합니다.\n AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Secondary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다.     HANA 설치 파일을 S3로 부터 다운 받기 위해 awscli 를 설치합니다.\n 자세한 설치 방법은 옆 링크를 참고 하시기 바랍니다. Install the AWS CLI version 2 on Linux root 유저로 접속 후, 최신 버전의 AWS CLI를 아래와 같이 설치 합니다  sudo su - cd /hana/shared curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip ./aws/install  aws cli 설치를 아래와 같이 확인 합니다  aws --version   HANA 설치 파일을 아래와 같이 다운 받은 후 압축을 해제 합니다.\n 실습 가이드에서 공유 받은 S3 URL로 부터 설치 파일을 다운 받습니다.(e.g s3://sap-immsersionday-hol1/hanadb)  mkdir -p /hana/shared/hanadb cd /hana/shared/hanadb aws s3 cp s3://sap-immsersionday-hol1/hanadb . --recursive  HANA 설치 파일 압축을 아래와 같이 해제 합니다.  unrar x 51053381_part1.exe   HANA DB를 설치 합니다.\n  주요 설치 옵션은 아래와 같습니다.\n System ID : QAS Instance Number : 10 Master Password : Init12345! Restrict maximum memory allocation : y Enter Maximum Memory Allocation in MB [63615]: 32768    hdblcm 을 사용해서 HANA DB를 설치 합니다.\n  cd /hana/shared/hanadb/51053381/DATA_UNITS/HDB_LCM_LINUX_X86_64 ./hdblcm   설치 옵션을 아래와 같이 입력 합니다.\n choose an action[install] : 2  Select additional components for installation[server] : 2  Install Path와, Host Name 등은 Default 설정을 사용하고, System ID는 QAS, Instance Number는 10 으로 입력합니다.  System usage는 test로 2 선택합니다, memory 설정을 32769 MB 및 패스워드를 Init12345! 입력하고 나머지는 Default 설정을 사용합니다.  Do you want to continue? (y/n) y 로 입력합니다.     설치가 완료되면 아래와 같은 결과를 확인하실 수 있습니다.     QAS HANA DB 설치를 확인 합니다.\n qasadm 유저로 스위치 하고, HDB 인스턴스가 정상적으로 기동 중인지 확인 합니다.  su - qasadm HDB info    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab5/lab5-3/",
	"title": "Task 03. Change Config of the Cluster for QAS system",
	"tags": [],
	"description": "",
	"content": "\rTask 03에서는 기존 Cluster 구성을 변경하여 기존 HANA Database와 새로 추가된 QAS HANA Database를 관리할 수 있도록 변경할 것입니다.\n\r Cost Optimized 시나리오 변경을 위해서는 아래와 같이 Cluster의 HANA DB 리소스에 대한 PREFER_SITE_TAKEOVER 옵션 변경이 필요합니다. 자세한 사항은 아래 링크를 참고하시기 바랍니다.  SAP HANA High Availability Cluster for the AWS Cloud - Setup Guide        Cluster 설정을 위해 sechana, prihana 노드의 DB 모두 stop 합니다.\n  Session Manager를 통해 sechana에 접속합니다.\n  AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Secondary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다.   QAS DB를 Stop 합니다.\n  sudo su - qasadm HDB stop exit  HDB DB를 Stop 합니다.  sudo su - hdbadm HDB stop exit   sechana 접속을 유지하고, Session Manager를 통해 prihana에 접속합니다.\n AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Primary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다.  HDB DB를 Stop 합니다.  sudo su - hdbadm HDB stop exit     prihana에서 root 유저로, Cluster 의 HANA DB 리소스에 대한 PREFER_SITE_TAKEOVER 옵션을 true 에서 false로 변경합니다.\n prihana의 유저를 root로 스위치 후, 스니펫 생성을 위한 폴더를 생성 합니다  sudo su - mkdir -p /hana/shared/script/ cd /hana/shared/script/  VI 편집기를 이용하여, 아래와 같이 스니펫 생성을 생성 합니다.  vi crm-SAPHana-update.txt primitive rsc_SAPHana_HDB_HDB00 ocf:suse:SAPHana \\  operations $id=rsc_sap_HDB_HDB00-operations \\  op start interval=0 timeout=3600 \\  op stop interval=0 timeout=3600 \\  op promote interval=0 timeout=3600 \\  op monitor interval=60 role=Master timeout=700 \\  op monitor interval=61 role=Slave timeout=700 \\  params SID=HDB InstanceNumber=00 PREFER_SITE_TAKEOVER=false DUPLICATE_PRIMARY_TIMEOUT=7200 AUTOMATED_REGISTER=true  생성한 스니펫을 crm에 반영 합니다.  crm configure load update crm-SAPHana-update.txt  업데이트 된 crm 설정을 확인 합니다.  crm config show   prihana에서 root 유저로, Cluster에 QAS HANA DB를 위한 리소스를 등록 합니다.\n VI 편집기를 이용하여, 아래와 같이 스니펫 생성을 생성 합니다.  vi crm-qas.txt primitive rsc_SAP_QAS_HDB10 ocf:heartbeat:SAPDatabase \\  params DBTYPE=\u0026#34;HDB\u0026#34; SID=\u0026#34;QAS\u0026#34; InstanceNumber=\u0026#34;10\u0026#34; \\  MONITOR_SERVICES=\u0026#34;hdbindexserver|hdbnameserver\u0026#34; \\  op start interval=\u0026#34;0\u0026#34; timeout=\u0026#34;600\u0026#34; \\  op monitor interval=\u0026#34;120\u0026#34; timeout=\u0026#34;700\u0026#34; \\  op stop interval=\u0026#34;0\u0026#34; timeout=\u0026#34;300\u0026#34; \\  meta priority=\u0026#34;100\u0026#34;  생성한 스니펫을 crm에 반영 합니다.  crm configure load update crm-qas.txt   prihana에서 root 유저로, 새로 생성한 QAS HANA DB 리소스 제약사항을 설정 합니다.\n VI 편집기를 이용하여, 아래와 같이 스니펫 생성을 생성 합니다.  vi crm-cs-qas.txt location loc_QAS_never_on_prihana rsc_SAP_QAS_HDB10 -inf: prihana colocation col_QAS_never_with_AWS_IP -inf: rsc_SAP_QAS_HDB10:Started \\ res_AWS_IP order ord_QASstop_before_HDB-promote inf: rsc_SAP_QAS_HDB10:stop \\ msl_SAPHana_HDB_HDB00:promote  생성한 스니펫을 crm에 반영 합니다.  crm configure load update crm-cs-qas.txt  업데이트 된 crm 설정을 확인 합니다.  crm config show   prihana에서 root 유저로, Cluster Maintenance 모드를 해제하고 정상적으로 설정 되었는지를 모니터링 합니다.\n Cluster Maintenance 모드를 해제합니다.  crm node ready prihana crm node ready sechana  Cluster 상태를 모니터링 합니다. QAS HANA DB 리소스가 정상적으로 Started 되었는지 확인 합니다.  crm_mon -rfn1  HSR 상태를 모니터링 합니다. sechana의 status가 SOK 상태인지 확인 합니다.  SAPHanaSR-showAttr   sechana에서 qasadm 유저로, QAS HANA DB가 정상적으로 기동 되었는지 확인 합니다.\n HDB process 들이 정상적으로 기동 되었는지 확인 합니다.  sudo su - qasadm HDB info    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/lab5/lab5-4/",
	"title": "Task 04. Test Cluseter",
	"tags": [],
	"description": "",
	"content": "\rTask 04에서는 설정은 변경 한 Cluster를 테스트하여 시나리오에 따라 정상 동작 하는지 확인합니다.\n\r  첫번째 시나리오는 prihana의 DB를 crash 했을 때, Cluster가 어떻게 동작 하는지 확인 합니다.    Session Manager를 통해 prihana에 접속합니다.\n  AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다.\n  HANA-HDB-Primary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.   Session Manager 를 선택하고, Connect 버튼을 누릅니다.   hdbadm 유저로 스위치 합니다. HDB DB를 Kill 합니다.\n  sudo su - hdbadm HDB kill -9 exit  root 유저로 스위치 합니다. Cluster 상태를 모니터링 합니다. 리소스 상태가 어떻게 변했는지 확인 합니다.  sudo su - crm_mon -rfn1  HSR 상태를 모니터링 합니다. sechana의 status가 SOK 상태인지 확인 합니다.  SAPHanaSR-showAttr  OS가 정상 상태이고, PREFER_SITE_TAKEOVER 옵션을 false로 변경 하였기 때문에 Take over 되지 않고, prihana에서 DB가 재 기동 되었습니다.     두번째 시나리오는 prihana의 OS를 crash 했을 때, Cluster가 어떻게 동작 하는지 확인 합니다.\n prihana의 OS를 fast reboot 합니다.  sudo su - echo \u0026#39;b\u0026#39; \u0026gt; /proc/sysrq-trigger exit   Session Manager를 통해 sechana에 접속합니다.\n AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Secondary 인스턴스를 선택하고, Action을 선택하고, Connect을 선택 합니다.  Session Manager 를 선택하고, Connect 버튼을 누릅니다.     sechana에서 Cluster 상태를 모니터링 합니다. 리소스 상태가 어떻게 변했는지 확인 합니다.\n  sudo su - crm_mon -rfn  prihana 시스템이 Down 되었기 때문에, 정상적으로 sechana로 Take Over 되었습니다. 정상적인 서비스를 위해 QAS 시스템을 내리고, 메모리도 정상 서비스 할 수 있도록 Memory limit 설정을 해제 하였습니다.  sudo su - hdbadm cat /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini   prihana 원복 합니다. 그리고 prihana가 정상이면, QAS 시스템 사용을 위해 Take Back을 수행합니다. sechana의 HDB의 global.ini 설정을 원복합니다.\n  prihana 인스턴스를 Start 시킵니다.\n AWS Management Console에 로그인 한 뒤 EC2 Instance Console에 접속 합니다. HANA-HDB-Primary 인스턴스를 선택하고, Action 을 선택하고, Instance state 에서 Start Instance 를 선택 합니다.     sechana에서 root 유저로 스위치 후, prihana 인스턴스가 정상적으로 기동 후 HDB HANA 리소스가 Slave 로 보이는지 확인합니다.\n  sudo su - crm_mon -rfn1  sechana에서, HSR 상태를 체크하여 prihana status가 SOK 인지 확인 합니다.  SAPHanaSR-showAttr  sechana에서, prihana로 Take Back을 수행합니다. node를 standby 로 변경하여 리소스 모두 prihana로 옮깁니다.  crm node standby sechana  prihana의 HDB HANA 리소스가 Master 로 전환되었는지 확인합니다.  crm_mon -rfn1  sechana에서 hdbadm 유저로 스위치 후, VI 편집기를 이용하여 global.ini 원복 합니다.  sudo su - hdbadm vi /usr/sap/HDB/SYS/global/hdb/custom/config/global.ini [system_replication] ... preload_column_tables = false #Add-on [memorymanager] global_allocation_limit = 24576  sechana에서 root 유저로 스위치 후, node를 online 시킵니다.  sudo su - crm node online sechana  sechana의 HDB HANA 리소스가 Slave 로 전환되고, QAS HANA 리소스가 Started 되었는지 확인합니다.  crm_mon -rfn1  HSR 상태를 체크하여 sechana status가 SOK 인지 확인 합니다.  SAPHanaSR-showAttr    "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/credits/",
	"title": "Credit",
	"tags": [],
	"description": "",
	"content": "컨텐츠 최초제작 기여자  AWS CS Korea SA  sejun@    컨텐츠 업데이트 기여자  AWS CS Korea SA  sejun@ jinuklee@ leesangg@    소프트웨어 기여자 오픈 소스 소프트웨어를 더 좋게 만들어 주시는 기여자 분들께 감사드립니다!\n@vjeantet의 docdock, hugo-theme-learn의 분기에 대한 업적에 특별히 감사드립니다. 이 테마의 v2.0.0은 그의 작업으로 부터 영감 받았습니다.\n패키지와 라이브러리  SAP IMMERSION DAY - 이 워크샵을 통해 AWS에서 SAP HANA 를 설치하고 운영하는 방법에 대해 알아봅니다. SAP HANA Quick Start - This Quick Start helps you rapidly deploy fully functional SAP HANA systems on the AWS Cloud, following best practices from AWS and SAP. SAP HANA SR Cost Optimized Scenario - This guide provides detailed information about installing and customizing SUSE Linux Enterprise Server for SAP Applications for SAP HANA system replication in the cost optimized scenario  도구  Netlify - Continuous deployement and hosting of this documentation Hugo  "
},
{
	"uri": "https://master.d24ltnpdigr6ti.amplifyapp.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]